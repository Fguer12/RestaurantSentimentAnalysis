# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iF5tNNoR2lhqVU4bNgQL1viJa8n6mv0Q
"""

# install mglearn: 
!pip install mglearn
# import libraries required:  
import os
import glob
from pprint import pprint

import sklearn
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from tqdm.auto import tqdm

import io

"""# Data import and Data preparation"""

# upload Restaurant_Reviews.tsv from your local drive  

from google.colab import files
uploaded = files.upload()

# Saving the dataset in a Panda Dataframe  

df = pd.read_csv("Restaurant_Reviews.tsv", sep='\t')

df.head(100)

# show row length to see how much data we have
len(df)

"""# Building a sentiment classifier"""

#New column created with help of a mapping from the values in the "Liked" column
df['Sentiment'] = df.Liked.map({
 0: 'Negativ',
 1: 'Positive'
    })
df.head(10)

# drop the null values in the review column to have only clean data. check the new length
df.Review.isna().sum()
#keep the ones that are not not null
df = df[~df.Review.isna()]
len(df)

# show number of entries per sentiment to make sure we have a balanced dataset (roughly 50%)
import matplotlib.pyplot as plt; plt.style.use('seaborn')

fig = plt.figure(figsize=(8, 6))

df\
    .groupby('Sentiment')['Review']\
    .count()\
    .plot(kind='bar', ylim=0)

plt.show()

"""# Label transformation & Text normalisation"""

# Use LabelEncoder  to convert text labels into values/id's
from sklearn.preprocessing import LabelEncoder


lbl_enc = LabelEncoder()
lbl_enc.fit(sorted(df['Sentiment'].unique()))

# obtaining a numeric representation of an array 
df['sentiment_id'] = lbl_enc.transform(df['Sentiment'])

df.head(10)

""" Helper function for Text normalisation"""  
# Here we introduce the stemmatization function, which we use afterwards within the count vectorizer  
# We have to describe the function separately because Sklearn does not offer a native stemming function 

import nltk
from nltk.stem.porter import PorterStemmer
from textblob import TextBlob
import re
!pip install textblob
nltk.download('punkt')
SENT_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')


porter_stemmer = PorterStemmer()

# Use TextBlob function in order to stem the data of the review rext
def textblob_tokenizer(str_input):
    blob = TextBlob(str_input.lower())
    tokens = blob.words
    words = [token.stem() for token in tokens]
    return words

# Use NLTK's PorterStemmer
def stemming_tokenizer(str_input):
    words = re.sub(r"[^A-Za-z0-9\-]", " ", str_input).lower().split()
    words = [porter_stemmer.stem(word) for word in words]
    return words

    stemming_tokenizer("I went fishing to get fishes")
    textblob_tokenizer("I went fishing to get fishes")

""" Text normalisation and Model creation"""  

## NNOTE: In this code section we transform the review data into text data. Labels have already been encoded  
from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB


# Split data
train_index, test_index = next(StratifiedShuffleSplit(n_splits=1, test_size=0.2).split(df['Review'], df['sentiment_id']))

# Get data
train_df = df.iloc[train_index]
test_df = df.iloc[test_index]

# Get data
X_train = train_df['Review'].to_numpy()
X_test = test_df['Review']
y_train_labels = train_df['sentiment_id']
y_test_labels = test_df['sentiment_id']
print(type(X_train))

# Convert labels to numbers NOTE: Here we do not convert to labels because we have done so above!!!
y_train = y_train_labels.to_numpy()
y_test = y_test_labels
print(type(y_train))


# Use a CountVectorizer for bag-of-words
count_vect = CountVectorizer(min_df=5, ngram_range=(1, 2), stop_words='english', tokenizer=textblob_tokenizer)
X_train_counts = count_vect.fit_transform(X_train)
X_test_counts = count_vect.fit_transform(X_test)

# Use Tfidf to transform the counts into tfidf weights
tfidf_transformer = TfidfTransformer(sublinear_tf=True, norm='l2')
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)

# TO GET FEATURES (get_feature_names()), YOU HAVE TO USE TfidfVectorizer
# This is the same as the two above
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# To test whether the transforamtion of the data worked, we insert the created variables as inputs to the multinomialNB to check if it can create a model 
clf = MultinomialNB()
clf.fit(X_train_counts, y_train)

"""# Model training with other classifiers"""

# A reusable function that we can loop through for every single classifier
# import Classifiers
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC

# Import training utilities
from sklearn.feature_selection import chi2

# Import metrics
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix

#Here we create a standardised function that can be used for each classifier to create a model. this allows comparability between the effectiveness of the classifiers
def train(model, features, labels, num_cv):
    
    results = []
    
    kfold = StratifiedShuffleSplit(n_splits=num_cv, test_size=0.2)
    
    for train, test in kfold.split(features, labels):
        
        clf = sklearn.clone(model)
        
        X_train = features[train]
        X_test = features[test]
        y_train = labels[train]
        y_test = labels[test]

        clf.fit(X_train, y_train)
        
        y_pred = clf.predict(X_test)        
        
        a = accuracy_score(y_test, y_pred)
        p, r, f, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
        report = classification_report(y_test, y_pred)
        
        results.append({
            'accuracy': a,
            'precision': p,
            'recall': r,
            'fscore': f,
            'report': report
        })

    return results

#In this code part we loop through the function above with different classifiers!!   
#NOTE: This code has dependencies with the text representation, since we take the vectorized data from there  

# Define the different classifier that we want to usemodels
models = [
    MultinomialNB(),
    LogisticRegression(random_state=0, solver='liblinear', multi_class='auto'),
    RandomForestClassifier(n_estimators=200, max_depth=3),
    LinearSVC()
]
# trainings set is split 5 times and the results are appended to the back of the entries list  

num_cv = 5

entries = []

# Go over each Classifier
for model in models:
    model_name = model.__class__.__name__
    
# NOTE: the train x_train_tfid and y_train are variables we created earlier! We shall not overwrite them. review data is not vectorised and the model can not analyze it!!!!!!  

    results = train(model, X_train_tfidf, y_train, num_cv)
#Create areport for the test results and store it to a list.

    for fold_idx, metric in enumerate(results):
        entries.append({
            'model_name': model_name,
            'fold_idx': fold_idx,
            'accuracy': metric['accuracy'],
            'precision': metric['precision'],
            'recall': metric['recall'],
            'fscore': metric['fscore'],
            'report': metric['report']
        })

# convert list to a panda dataframe
cv_df = pd.DataFrame(entries)

cv_df

"""# Model evaluation"""

# reusable function to illustrate data with a certain type of box plot
import matplotlib.pyplot as plt; plt.style.use('seaborn')
import seaborn as sns

def plot_metric(df, metric):
    fig = plt.figure(figsize=(6, 8))

    sns.boxplot(x='model_name', y=metric, data=cv_df[['model_name', metric]])
    sns.stripplot(x='model_name', y=metric, data=cv_df[['model_name', metric]], 
                  size=8, jitter=True, edgecolor="gray", linewidth=2)
    plt.xticks(rotation=30, ha='right')
    plt.show()

#create a plot for the accuracy values
plot_metric(cv_df, 'accuracy')

#overarching model evaulation. we see that MultinomialNB is the most efficient one. hence we train the entire training data with SVC to see
# if we can increase the scores..
cv_df.groupby('model_name')[['accuracy', 'precision', 'recall', 'fscore']].mean()

"""# Model Optimisation"""

# We use the MultinomialNB and train on the WHOLE training set, without k-fold.
model = MultinomialNB()

model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

#we want to graphically illustrate our results in more detail. true positives, false negatives 
from sklearn.utils.multiclass import unique_labels
# First, we need a function to draw the confustion matrix
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Reds, ax=None):
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    classes = [classes[i] for i in unique_labels(y_true, y_pred)]
            
    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    if ax is None:
        ax = plt.gca()
    
    im = ax.imshow(cm, cmap=cmap)
    
    cb = ax.figure.colorbar(im, ax=ax)
    
    tick_marks = np.arange(len(classes))
    
    ax.set_xticks(tick_marks)
    ax.set_yticks(tick_marks)
    
    ax.set_xticklabels(classes, rotation=0, ha='right', rotation_mode='anchor')
    ax.set_yticklabels(classes, rotation=0)
    
    ax.set_xlabel('Predicted class')
    ax.set_ylabel('True class')
    
    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    
    #fig.tight_layout()
    plt.grid(False)
    
    return ax

fig, ax = plt.subplots(figsize=(12, 12))

plot_confusion_matrix(y_test, y_pred, classes=lbl_enc.classes_, normalize=True)

ax.set_title('Confusion Matrix (Normalized Values)')
plt.show()

fig, ax = plt.subplots(figsize=(12, 12))

plot_confusion_matrix(y_test, y_pred, classes=lbl_enc.classes_, normalize=True)

ax.set_title('Confusion Matrix (Normalized Values)')
plt.show()